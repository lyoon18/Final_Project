---
title: "Final Project: What's Cooking?"
author: "Vickie Ip, Brendan Seto and Leonard Yoon"
date: "December 22, 2017"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: false
    df_print: kable
editor_options: 
  chunk_output_type: console
---

```{r, message=FALSE, warning=FALSE, fig.width=8, fig.height=4.5, echo=FALSE}
# install.packages("jsonlite")
library(tidyverse)
library(jsonlite)
library(gridExtra)
library(randomForest)

# Some customization.  You can alter or delete as desired (if you know what you are doing).
knitr::opts_chunk$set(
  cache = TRUE,
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
```

# Abstract

**Overall Goal:** Predict type of cuisine based on ingredients

**Project Goal:** Explore decision trees with categorical variables.  Specifically, the variation of runtime and accuracy with different data reduction strategies.  

**Results:**

Model             | Accuracy (% correct) | Runtime   | Number of Variables Used |
------------------|----------------------|-----------|--------------------------|
Original Random Forest | 66.7% | > 2 hours | 6,715                  |
CART             | 46.9%  | 10 minutes  | 6,715                     |
RF: No Identifying | 68.0% | ~ 1.5 hours | 4,115                  |
RF: String Cleaning | 69.4% | 15 minutes | 1,190                  |

As shown in the table above, 



# Exploratory Data Analysis
  
## Data
  
```{r, cache=TRUE}
food <- read_csv("data/recipe.csv")
```

```{r, cache=TRUE}
long <- fromJSON("data/train.json") %>% 
  unnest()
```

Our data came from the *"What's Cooking?"* Kaggle competition.  Each of the 39,774 rows was a recipe, with columns listing the different ingredients.  For some of the analysis we used one-hot-encoding to transform the data to binary indicators.  This lead to our final dataset consisting of 6,715 variables, one for each unique ingredient.  

```{r, echo=FALSE}
long1a <- long %>% 
  group_by(cuisine) %>% 
  summarise(num = n()) %>% 
  mutate(percent = num/nrow(food)*100)

ggplot(long1a, aes(x=reorder(cuisine, -num))) + 
  geom_bar(aes(y=num, fill = "orange3"), color = "black", stat = "identity")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 60, hjust = 1))+
  labs(list(title = "Cuisines By Number of Ingredients", x = "Cuisine", y = "Total # of Ingredients"))+
  guides(fill=FALSE)
```

There were 20 total cuisines represented in our dataset.  Of these, Italian was the most prevalent, making up nearly 20% of the total recipes. 

## One Hot Encoding

```{r}
sample_n(long,5)
```

As you can see from above, the original dataset was structured in that each row represented an ingredient in a recipe. This format makes it very difficult for us to run classification tree models so we decided to convert all the ingredients into binary indicators using One Hot Encoding.


```{r, message=FALSE, warning=FALSE, fig.width=8, fig.height=4.5, echo=FALSE}
# Create dummy variable
dummies = model.matrix(~long$ingredients)
food <- cbind(long[,c(1,2)], dummies) # this takes a few minutes

# Group individual recipe
food <- food[,-3] %>% group_by(id, cuisine) %>% summarise_all(sum)
```

```{r, message=FALSE, warning=FALSE, fig.width=8, fig.height=4.5, echo=FALSE}
# Make names pretty
colnames(food)[5]
substr(colnames(food)[5], 17, stop = nchar(colnames(food)[5]))

names <- c("id","cuisine",sapply(colnames(food)[c(-1,-2)], function(x) substr(x, 17, stop = nchar(x))))
colnames(food) <- names
```

## Identifying Ingredients

```{r}
# Find the Identifying Ingredients
unique <- long %>% 
  group_by(ingredients) %>% 
  mutate(kinds = n_distinct(cuisine), n = n())

data <- unique %>% filter(kinds!=1)

special <- unique %>% filter(kinds == 1)
#n_distinct(special$ingredients)
#n_distinct(special$id)
```

Of the 6,715 ingredients, 2,597 only appear in one type of cuisine.  We can use this information to perform an initial screening of our data, classifying any dish that has one of these ingredients.  This eliminates 5,438 of our recipes in the training set and 

Some cuisines benefit more from this filtering.  14% of all recipes have an identifying ingredient, but 33% of brazilian and 26% of mexican cuisine did.  

```{r}
# Number of Recipes with identifying ingredients
s<- special %>% 
  group_by(id, cuisine) %>% 
  summarise() %>% 
  group_by(cuisine) %>% 
  summarise(n= n()) 

# Number of Recipes Total
num <- unique %>% 
  group_by(id, cuisine) %>% 
  summarise(n = n()) %>% 
  group_by(cuisine) %>% 
  summarise(recipes = n())

# Calculate as percentage of total recipes
s <- merge(s, num, by = "cuisine")
s <- s %>% 
  mutate(percent = n/recipes*100)
total <- s %>% summarise(n = sum(n), recipes = sum(recipes), percent = n/recipes*100)

g <- ggplot(s, aes(x=reorder(cuisine, -n))) + 
  geom_bar(aes(y=n), fill = "orange3", color = "black", stat = "identity")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 60, hjust = 1))+
  labs(list(title = "Total Number", x = "Cuisine", y = "Dishes with Identifying Ingredients"))

p <- ggplot(s, aes(x=reorder(cuisine, -n))) + 
  geom_bar(aes(y=percent), fill = "skyblue", color = "black",stat = "identity")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 60, hjust = 1))+
  labs(list(title = "Percent", x = "Cuisine", y = "Percent with Identifying Ingredients"))+
  geom_hline(yintercept = total$percent, linetype = "longdash")

grid.arrange(g,p, ncol=2, top = "Recipes with Identifying Ingredients")
```

## Most Commonly Used Ingredients

```{r}
# Wordcloud
library(dplyr)
library(wordcloud)
library(tidyverse)
library("RColorBrewer")

#no_special <- unique %>% filter(kinds == 20)
#no_special <- no_special[with(no_special, order(-n)), ]

freq <- sort(table(long$ingred), decreasing = T)
freq <- freq[1:80]
freq <- as.data.frame(freq)
colnames(freq) <- c("Ingredient", "Frequency")
set.seed(101)
wordcloud(words = freq$Ingredient, freq = freq$Frequency, min.freq=1, random.order=FALSE, rot.per = 0, colors=brewer.pal(8,"Dark2"))
```

As the wordcloud shows, the most commonly used ingredient is salt. Since 189,440 of these ingredients appear in all 20 cuisines, it might be a suitable idea to remove these ingredients from our training set.


## Simplifying Ingredients

Many of the ingredients listed in the data are variations of the same.  For instance:

```{r}
colnames(food)[c(4,5)]
```

It does not make sense to consider these two as distinct ingredients and their addition contributes to a very slow runtime for our model.  In an attempt to reduce our dimension size, we will try to combine these as best we can in a quick, automated way.  

```{r}
common <- data %>% filter(!stringr::str_detect(ingredients, " "))
#n_distinct(common$ingredients)
commonI <- common[!duplicated(common$ingredients), "ingredients"]
```

We start by finding ingredients that consist of only one word.  We consider these ingredients "fundamental", unable to be simplified more.  For instance, both of the above variables should really be considered *"spinach"*. 

```{r}
allI <- data[!duplicated(data$ingredients), "ingredients"]
# Need to avoid non-substrings like oil -> soil
commonI <- ungroup(commonI)
commonI2 <- rbind(commonI %>% mutate(ingredients = paste(ingredients," ", sep="")),commonI %>% mutate(ingredients = paste(" ",ingredients, sep="")))

similarity <- c()
for(i in allI$ingredients){
  for(j in commonI2$ingredients){
    if(grepl(j,i)) similarity <- rbind(similarity,c(i,j))
  }
}

similarity <- as.data.frame(similarity)
similarity[,2] <- gsub("\\s", "", similarity[,2])
colnames(similarity) <- c("long","short")

### There are some (1186) ingredients that map to multiple short versions.  We shall assume that the longer short name is most descriptive.  A better analysis would go through them manually and decide
similarity1 <- similarity %>% group_by(long) %>% 
  mutate(n = n()) %>% 
  filter(n>1)
#n_distinct(similarity1$long)

similarity2 <- similarity %>% 
  group_by(long) %>% 
  slice(which.max(nchar(short)))

#n_distinct(similarity2$long)
#n_distinct(similarity2$short)
```

We have now mapped nearly 3,000 variables to 418 (total number of ingredients is 1,193).  This is a very significent reduction, even more than the identifying ingredient filter!

```{r}
commons <- merge(data, similarity2, by.x = "ingredients", by.y = "long", all.x = TRUE)

commons <- commons %>% mutate(ingredients = ifelse(!is.na(short),short,ingredients)) %>% 
  select(ingredients,id, cuisine)
```



# Models

## CART

First, some more data cleaning. Five cuisines didn't show up enough to be predicted by CART in any scenario that we tried (and we tried many!). We combined two of them (British and Irish) to form a larger category (UK) that can be predicted by CART and got rid of the other three cuisines.

```{r}
food$cuisine <- replace(food$cuisine, food$cuisine == "irish"|food$cuisine == "british", "UK") # add irish to british

food <- food %>% 
  filter(cuisine != "brazilian") %>% 
  filter(cuisine != "russian") %>% 
  filter(cuisine != "jamaican")  # These cuisines did not appear frequent enough
```

Here's the model that we ended up using. See crossvalidation below to figure out why.

```{r}
library(rpart)
dataA <- food[sample(nrow(food),nrow(food)*.5),] 
dataB <- anti_join(food, dataA, by = "id")

cp_star <- .0013 # see CV section for this
model_formula <- as.formula(cuisine ~ . - id)
tree_parameters <- rpart.control(maxdepth = 20, cp = cp_star, minsplit = 1, minbucket = 1) 
model_CART <- rpart(model_formula, data = dataA, control=tree_parameters) # takes 10 minutes

# Score/error
y_hat <- model_CART %>% 
  predict(newdata=dataB, type="class") # takes about a minute
conf.matrix <- MLmetrics::ConfusionMatrix(y_true = dataB$cuisine, y_pred = y_hat)
t <- as.data.frame(conf.matrix) %>% 
  group_by(y_true) %>% 
  mutate(total = sum(Freq)) %>% 
  ungroup() %>% 
  mutate(percent = Freq/total)

ggplot(t %>% filter(percent>0.1)) +
  geom_point(aes(y_pred, y_true, size = percent, color = percent)) +
  theme_bw()+
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +scale_y_discrete(name="True", limits = rev(levels(t$y_true))) +
  labs(list(title = "CART Confusion Matrix", x = "Guess"))+
  guides(color = FALSE, size = FALSE)+scale_colour_gradient(low = "pink", high = "red4")
```

## Crossvalidation of ultimate model
```{r, message=FALSE, warning=FALSE, fig.width=8, fig.height=4.5, echo=FALSE}
# k = 2 fold crossvalidation

# Consider a range of cp. I chose this through trial and error.
cp_vector <- seq(from = .0007, to = .0025, by = .0003)

# Save results here
results <- data_frame(
  cp = cp_vector,
  MLL = 0 # multi class log loss (MLL)
)

model_formula <- as.formula(cuisine ~ . - id)

set.seed(18)
for(i in 1:length(cp_vector)){
  # Note: these two datasets are disjoint
  dataA <- food[sample(nrow(food),nrow(food)*.5),] 
  dataB <- anti_join(food, dataA, by = "id")
  
  # maxdepth being large is what causes the thing to take so long
  # as maxdepth increases, we get predictions of more cuisines
  # maxdepth = 20 is doing very well and takes about 10 minutes to run

  # Fit model onto A, get predictions for B, compute MLL for B  
  tree_parameters <- rpart.control(maxdepth = 20, cp = cp_vector[i], minsplit = 1, minbucket = 1) 
  model_CART <- rpart(model_formula, data = dataA, control=tree_parameters) # takes a LONG time

  p_hat_matrix <- model_CART %>% 
    predict(type = "prob", newdata = dataB) # takes about a minute

  MLLB <- MLmetrics::MultiLogLoss(y_true = dataB$cuisine, y_pred = p_hat_matrix)

  # Fit model onto B, get predictions for A, compute MLL for A
  
  tree_parameters <- rpart.control(maxdepth = 20, cp = cp_vector[i], minsplit = 1, minbucket = 1) 
  model_CART <- rpart(model_formula, data = dataB, control=tree_parameters) # takes a LONG time

  p_hat_matrix <- model_CART %>% 
    predict(type = "prob", newdata = dataA) # takes about a minute
  
  MLLA <- MLmetrics::MultiLogLoss(y_true = dataA$cuisine, y_pred = p_hat_matrix)
  
  # Take mean
  results$MLL[i] <- (MLLA + MLLB)/2
}

optimal <- results %>% 
  arrange(MLL) %>% 
  slice(1)
cp_star <- optimal$cp
MLL_star <- optimal$MLL
optimal %>% 
  kable(digits=4)

ggplot(results, aes(x=cp, y=MLL)) +
  geom_point() +
  labs(x="CART Complexity Parameter", y="Multi-Class Log Loss", title="Crossvalidation estimates of Multi-Class Log Loss for CART model") +
  geom_vline(xintercept = cp_star, col="red")
```


## Random Forest

### Trims


#### No Trim
```{r, cache=TRUE, eval=FALSE}

#### No Trim (Baseline Model)

recipe <- food
names(recipe) <- make.names(names(recipe))
recipe <- recipe[, !duplicated(colnames(recipe), fromLast = TRUE)]
recipe[] <- lapply(recipe, factor)


sub <- recipe[sample(nrow(recipe),nrow(recipe)*.8),]
sub2 <- anti_join(recipe, sub, by = "id")

fit <- randomForest(cuisine ~. - id, data = sub)
```

```{r warning = FALSE, eval=FALSE}
predictd <- predict(fit, sub2)

t <- data_frame(guess = as.character(predictd), true = as.character(sub2$cuisine)) %>% mutate(right = ifelse(guess == true,1,0))

acc1 <- round(mean(fit_large$right)*100,1)

trf <- as.data.frame(cbind(t,model.matrix(~t$guess)))[,c(-1,-3,-4)]%>% group_by(true) %>% summarise_all(mean)
colnames(trf)[2:ncol(trf)] <- stringr::str_sub(colnames(trf)[2:ncol(t2)],8,-1)

#write_csv(trf, "RF.csv")
```

```{r, warning=FALSE}
trf <- read_csv("RF.csv")
fit_large_graph2 <- as.data.frame(trf) %>% gather(key = guess, value = percent, 2:nrow(trf))

ggplot(fit_large_graph2 %>% filter(percent>=.05)) +
  geom_point(aes(guess, as.character(true), size = percent, color = percent))+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  scale_y_discrete(name="True", limits = rev(levels(as.factor(fit_large_graph2$true)))) +
  labs(list(title = paste("Random Forest: Acc: ", acc1,"%, Runtime: >2 hr, Var: ",ncol(food), sep=""), x = "Guess"))+
  guides(color = FALSE, size = FALSE)+
  scale_colour_gradient(low = "pink", high = "red4")
```


#### No Identifying

```{r, eval=FALSE}
identify <- filter(unique, kinds != 1)


# Create dummy variable
dummies = model.matrix(~identify$ingredients)
dumb <- cbind(identify[,c(1,2)], dummies)

# Group individual recipe
dumb <- dumb[,-3] %>% group_by(id, cuisine) %>% summarise_all(sum)
```

```{r, cache=TRUE, eval=FALSE}
recipe <- dumb
names(recipe) <- make.names(names(recipe))
recipe <- recipe[, !duplicated(colnames(recipe), fromLast = TRUE)]
recipe[] <- lapply(recipe, factor)


sub <- recipe[sample(nrow(recipe),nrow(recipe)*.8),]
sub2 <- anti_join(recipe, sub, by = "id")

fit3 <- randomForest(cuisine ~. - id, data = sub)
fit_large3 <- fit3$confusion

predictedd1 <- predict(fit3, sub2)
t1 <- data_frame(guess = predictedd1, true = sub2$cuisine)
t1 <- t1 %>% mutate(yes = ifelse(guess==true,1,0))
acc2 <- round(mean(t1$yes)*100,1)
t2 <- model.matrix(~t1$guess)
cajun <- rowSums(t2)
cajun <- ifelse(cajun == 1,1,0)

t <- cbind(as.character(t1$true), cajun, t2[,-1])
colnames(t) <- c("true","brazilian","british","cajun_creole","chinese","filipino","french","greek","indian","irish","italian","jamaican","japanese","korean","mexican","moroccan","russian","southern_us","spanish","thai","vietnamese")
t <- as.data.frame(t)
t[2:21] <- sapply(t[2:21],as.character)
t[2:21] <- sapply(t[2:21],as.numeric)

t <-   t %>% 
  group_by(true) %>% 
  summarise_all(mean, na.rm = TRUE)

write_csv(t, "trim_NoI_RF.csv")
```

```{r}
noI <- read_csv("trim_NoI_RF.csv")
fit_large_graph3 <- noI %>% gather(key = guess, value = percent, 2:ncol(noI))


ggplot(fit_large_graph3 %>% filter(percent>0.1)) +
  geom_point(aes(guess, true, size = percent, color = percent)) +
  theme_bw()+
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  scale_y_discrete(name="True", limits = rev(levels(fit_large_graph$true))) +
  labs(list(title = paste("Trimmed Random Forest 1: Acc: ",acc2,"%, Runtime: ~1hr 20min, Var: ",ncol(sub), sep = ""), x = "Guess"))+
  guides(color = FALSE, size = FALSE)+
  scale_colour_gradient(low = "pink", high = "red4")
```

#### Fewer Categories

```{r}
plenty <- long1a %>% filter(percent >=2.5)

identify <- unique %>% 
  group_by(id) %>% 
  filter(id %in% train$id) %>% 
  mutate(keep = ifelse(min(kinds)==1,0,1)) %>% 
  ungroup() %>% 
  filter(keep == 1 & kinds != 20 & cuisine %in% plenty$cuisine) %>% 
  select(id, cuisine, ingredients)


# Create dummy variable
dummies = model.matrix(~identify$ingredients)
dumb <- cbind(identify[,c(1,2)], dummies)

# Group individual recipe
dumb <- dumb[,-3] %>% group_by(id, cuisine) %>% summarise_all(sum)
```

```{r, cache=TRUE}
recipe <- dumb
names(recipe) <- make.names(names(recipe))
recipe <- recipe[, !duplicated(colnames(recipe), fromLast = TRUE)]
recipe[] <- lapply(recipe, factor)

fit2 <- randomForest(cuisine ~. - id, data = recipe)
fit_large2 <- fit2$confusion
```

```{r}
test <- unique %>% 
  group_by(id) %>% 
  filter(id %in% testid$id) %>% 
  select(id, cuisine, ingredients)


# Create dummy variable
dumb_test <- model.matrix(~test$ingredients)
dummy <- data.frame(test[,c(1,2)], dumb_test)

# Group individual recipe
dumb <- dumb[,-3] %>% group_by(id, cuisine) %>% summarise_all(sum)
colnames(dumb) <- make.names(colnames(recipe))
dumb <- dumb[, !duplicated(colnames(dumb), fromLast = TRUE)]
dumb[] <- lapply(dumb, factor)

dumb2 <- dumb[,c(-1,-2)]


predictedd2 <- predict(fit2, as.matrix(dumb2))
                       
t <- data_frame(guess = predictedd, true = sub2$cuisine)
t2 <- model.matrix(~t$guess)
cajun <- rowSums(t2)
cajun <- ifelse(cajun == 1,1,0)

t3 <- cbind(as.character(t$true), cajun, t2[,-1])
colnames(t3) <- c("true","cajun_creole","chinese","french","greek","indian","italian","japanese","mexican","southern_us","thai")
t3 <- as.data.frame(t3)
t3[2:11] <- sapply(t3[2:11],as.character)
t3[2:11] <- sapply(t3[2:11],as.numeric)

t3 <-   t3 %>% 
  group_by(true) %>% 
  summarise(total = n(),cajun_creole = sum(cajun_creole)/total,chinese = sum(chinese)/total, french = sum(french)/total, greek = sum(greek)/total, indian = sum(indian)/total, italian = sum(italian)/total, japanese = sum(japanese)/total, mexican = sum(mexican)/total, southern_us = sum(southern_us)/total, thai = sum(thai)/total)
t3 <- t3[,-2]
write_csv(t3, "trim2RF.csv")
```

```{r}
fit_large_graph2 <- t3 %>% gather(key = guess, value = percent, 2:11)

ggplot(fit_large_graph2 %>% filter(percent>=.10)) +
  geom_point(aes(guess, as.character(true), size = percent, color = percent))+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  scale_y_discrete(name="True", limits = rev(levels(fit_large_graph2$true))) +
  labs(list(title = "Trimmed Random Forest 2: Err: 23.6%, Runtime: ~1hr 15 min", x = "Guess"))+
  guides(color = FALSE, size = FALSE)+
  scale_colour_gradient(low = "pink", high = "red4")
```

#### Commons

```{r}
ids <- unique %>% group_by(id) %>% summarise
train <- sample_frac(ids, 0.8)
testid <- anti_join(ids, train, by = "id")

identify <- commons %>% filter(id %in% train$id)

# Create dummy variable
dummies = model.matrix(~identify$ingredients)
dumb <- cbind(identify[,c(2,3)], dummies)

# Group individual recipe
dumb <- dumb[,-3] %>% group_by(id, cuisine) %>% summarise_all(max)

# Fix column names
colnames(dumb) <- make.names(colnames(dumb))
colnames(dumb)[3:ncol(dumb)] <- c("seven.Up",stringr::str_sub(colnames(dumb)[4:ncol(dumb)],21,-1))
#sum(dumb[,c(-1,-2)] %>% summarise_all(max))
dumb[] <- lapply(dumb, factor)
```

```{r, cache=TRUE}
fitC <- randomForest(cuisine ~. - id, data = dumb)
# 15 min!!!
fit_largeC <- fitC$confusion
fitC_importance <- fitC$importance
```

```{r, cache=TRUE}
# Crossvalidation
test <- unique %>% 
  group_by(id) %>% 
  filter(id %in% testid$id) %>% 
  select(id, cuisine, ingredients)

test <- merge(test, similarity2, by.x = "ingredients", by.y = "long", all.x = TRUE)

test <- test %>% mutate(ingredients = ifelse(!is.na(short),short,ingredients)) %>% 
  select(ingredients,id, cuisine)

# Add indicators for ingredients in train but not test data

addI <- identify %>%select(ingredients) %>% group_by(ingredients) %>% 
  summarise(id=0, cuisine = "mexican")
test <- rbind(test, addI)

# Take out ingredients in test that are not in train

test1 <- test %>% filter(ingredients %in% identify$ingredients)


##### One hot encoding #####
# Create dummy variable
dumb_test <- model.matrix(~test1$ingredients)
dummy <- data.frame(test1[,c(2,3)], dumb_test)

# Group individual recipe
dumby <- dummy[,-3] %>% group_by(id, cuisine) %>% summarise_all(max)
# fix column names
colnames(dumby) <- make.names(colnames(dumby))
colnames(dumby)[3:ncol(dumby)] <- c("seven.Up",stringr::str_sub(colnames(dumby)[4:ncol(dumby)],18,-1))
#sum(dumby[,c(-1,-2)] %>% summarise_all(max))
dumby[] <- lapply(dumby, factor)

dumby2 <- dumby[,c(-2)]
all.equal(rownames(fitC_importance), colnames(dumby2)[-1])

predicteddC <- predict(fitC, as.matrix(dumby2))
                       
t <- data_frame(guess = predicteddC, true = dumby$cuisine)
acc <- t %>% mutate(acc = ifelse(guess == true, 1, 0)) 
acc <- round(mean(acc$acc)*100,1)
t2 <- as.data.frame(cbind(t,model.matrix(~t$guess)))[,c(-1,-3)] %>% group_by(true) %>% summarise_all(mean)
colnames(t2)[2:ncol(t2)] <- stringr::str_sub(colnames(t2)[2:ncol(t2)],8,-1)

write_csv(t2, "trimCRF.csv")
```

```{r}
fit_large_graph2 <- as.data.frame(t2) %>% gather(key = guess, value = percent, 2:20)

ggplot(fit_large_graph2 %>% filter(percent>=.05)) +
  geom_point(aes(guess, as.character(true), size = percent, color = percent))+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  scale_y_discrete(name="True", limits = rev(levels(fit_large_graph2$true))) +
  labs(list(title = paste("Trimmed RF Common Ingred: Acc: ", acc,"%, Runtime: ~15 min!!!, Var: ",ncol(dumb), sep=""), x = "Guess"))+
  guides(color = FALSE, size = FALSE)+
  scale_colour_gradient(low = "pink", high = "red4")
```

# Create submission

```{r}
kaggle <- fromJSON("data/train.json") %>% 
  unnest()

k <- model.matrix(~kaggle$ingredients)
ka <- cbind(kaggle[,c(1,2)], k) # this takes a few minutes

# Group individual recipe
kaggle <- food[,-3] %>% group_by(id, cuisine) %>% summarise_all(sum)
rm(k, ka)

# Add missing ingredients
addI <- long %>%select(ingredients) %>% group_by(ingredients) %>% 
  summarise(id=0, cuisine = "mexican")
kaggle <- rbind(kaggle, addI)

# First RF model

kRF <- kaggle[,-which(names(kaggle) %in% colnames(sub))]

predictK1 <- predict(fit, kaggle)

```


# Citations and references

Note: All citations and references must be included here.



# Supplementary materials

Note: Anything else you've tried that you'd like to include, but isn't essential to
the above, like other EDA's, other modeling approaches you've tried, etc. Please
set the R code chunk `eval=FALSE` here so that default is that R Markdown
doesn't run the code, but a user can flip this switch if they are curious.

If we get rid of Italian, then Southern US becomes the cuisine that is over-predicted.

```{r, eval=FALSE, message=FALSE, warning=FALSE, fig.width=8, fig.height=4.5, echo=FALSE}
food2 <- food %>% filter(cuisine != "italian")
sub_2 <- food2[sample(nrow(food2),nrow(food2)*.5),]
sub2_2 <- anti_join(food2, sub, by = "id")
model_CART_2 <- rpart(model_formula, data = sub_2, control=tree_parameters) # takes 10 minutes

# Score/error
p_hat_matrix_2 <- model_CART_2 %>% 
  predict(type = "prob", newdata = sub2_2)
MLmetrics::MultiLogLoss(y_true = sub2_2$cuisine, y_pred = p_hat_matrix_2)
y_hat_2 <- model_CART_2 %>% 
  predict(newdata=sub2_2, type="class")
MLmetrics::Accuracy(y_true = sub2_2$cuisine, y_pred = y_hat_2)
conf.matrix_2 <- MLmetrics::ConfusionMatrix(y_true = sub2_2$cuisine, y_pred = y_hat_2)
t2 <- as.data.frame(conf.matrix_2) %>% 
  group_by(y_true) %>% 
  mutate(total = sum(Freq)) %>% 
  ungroup() %>% 
  mutate(percent = Freq/total)

ggplot(t2 %>% filter(percent>0.1)) +
  geom_point(aes(y_pred, y_true, size = percent, color = percent)) +
  theme_bw()+
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +scale_y_discrete(name="True", limits = rev(levels(t$y_true))) +
  labs(list(title = "CART Confusion Matrix", x = "Guess"))+
  guides(color = FALSE, size = FALSE)+scale_colour_gradient(low = "pink", high = "red4")

```

If we get rid of Southern US, then Mexican becomes the cuisine that is over-predicted.

```{r, eval=FALSE, message=FALSE, warning=FALSE, fig.width=8, fig.height=4.5, echo=FALSE}
food3 <- food2 %>% filter(cuisine != "southern_us")
sub_3 <- food3[sample(nrow(food3),nrow(food3)*.5),]
sub2_3 <- anti_join(food3, sub, by = "id")
model_CART_3 <- rpart(model_formula, data = sub_3, control=tree_parameters) # takes 10 minutes

# Score/error
p_hat_matrix_3 <- model_CART_3 %>% 
  predict(type = "prob", newdata = sub2_3)
MLmetrics::MultiLogLoss(y_true = sub2_3$cuisine, y_pred = p_hat_matrix_3)
y_hat_3 <- model_CART_3 %>% 
  predict(newdata=sub2_3, type="class")
MLmetrics::Accuracy(y_true = sub2_3$cuisine, y_pred = y_hat_3)
conf.matrix_3 <- MLmetrics::ConfusionMatrix(y_true = sub2_3$cuisine, y_pred = y_hat_3)
t3 <- as.data.frame(conf.matrix_3) %>% 
  group_by(y_true) %>% 
  mutate(total = sum(Freq)) %>% 
  ungroup() %>% 
  mutate(percent = Freq/total)

ggplot(t3 %>% filter(percent>0.1)) +
  geom_point(aes(y_pred, y_true, size = percent, color = percent)) +
  theme_bw()+
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +scale_y_discrete(name="True", limits = rev(levels(t$y_true))) +
  labs(list(title = "CART Confusion Matrix", x = "Guess"))+
  guides(color = FALSE, size = FALSE)+scale_colour_gradient(low = "pink", high = "red4")
```

So it seems like there is no escape from this phenomenon of one category getting everything to predict to it! Not sure why it does this.

The table below was helpful for debugging some things. It shows all of the cuisines and how many ingredients in the data set are attributed to each cuisine.

```{r, eval=FALSE, message=FALSE, warning=FALSE, fig.width=8, fig.height=4.5, echo=FALSE}
table.of.stuff <- food$cuisine %>% table() %>% sort()
table.of.stuff <- as.data.frame(table.of.stuff)
# table.of.stuff <- table.of.stuff %>% filter(Freq > 1000)
table.of.stuff <- table.of.stuff %>% rename()
table.of.stuff
```


